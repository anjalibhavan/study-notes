# Recommender system points

Three parts: candidate generation, ranking, reranking (after ranking, you gotta rerank based on user feedback like i didnt like this post)

### Instagram explore recommendations

[http://patrickhalina.com/posts/ml-systems-design-interview-guide/#pre-interview-checklist](http://patrickhalina.com/posts/ml-systems-design-interview-guide/#pre-interview-checklist)

1. Account embeddings: we created a retrieval pipeline that focuses on account-level information rather than media-level. By building account embeddings, we’re able to more efficiently identify which accounts are topically similar to each other. Ig2vec treats account IDs that a user interacts with — e.g., a person likes media from an account — as a sequence of words in a sentence.
2. We define a distance metric between two accounts — the same one used in embedding training — which is usually cosine distance or dot product. Based on this, we do a KNN lookup to find topically similar accounts for any account in the embedding. KNN using FAISS.
3. Train classifier to predict set of acc’s topics based on embedding. Evaluate with human labeled topics. Thus we have a personalized set of candidates for each user.
4. Ranking distillation model to preselect candidates before complex ranking: We record the input candidates with features, as well as outputs, from our more complicated ranking models. The distillation model is then trained on this recorded data with a limited set of features and a simpler neural network model structure to replicate the results. Its objective function is to optimize for NDCG ranking loss over main ranking model’s output. We use the top-ranked posts from the distillation model as the ranking candidates for the later-stage high-performance ranking models.
5. All these things are used in final reco system. Cand. gen is done with seed accs (those which our user interacted w recently) and using acc embeddings technique and many sources like video, stories etc. Then ranking is done in three passes - distillation model, lightweight nn, deep nn. Then reranking based on stuff like “see fewer posts like this”.
6. Wide and deep model

### Pinterest Pixie Recommendations

1. Pixie is a flexible, graph-based system for making personalized recommendations in real-time
2. One of the biggest challenges of our recommendation problem is figuring out how to narrow down the best Pin for the best person at the best time. This is where the graph-based recommender system comes in: we know a set of nodes that are already interesting to a Pinner, so we start graph traversal from there.
3. Pixie then finds the Pins most relevant to the user by applying a random walk algorithm for 100,000 steps. At each step, it selects a random neighbor and visits the node, incrementing node visit counts as it visits more random neighbors. 
4. Once the random walks are complete, we know the nodes which have been visited most frequently are the ones most closely related to the query node. Pixie continuously repeats this process in real-time as the data grows.
5. Optimizing performance: early stopping, graph pruning
6. Two-tower approach: It has a separate user tower and a Pin tower with a final dot product that computes the similarity between a given user and a given Pin. The Pin tower takes the features from the given Pin and generates a Pin embedding for it. The user tower takes the engagement history features as input and generates a user-specific embedding. Finally, we do a simple dot product based on the two embeddings as a measure of how likely the Pinner will engage with the Pin.
7. Instead of just looking at CTR, we look at a weighted average of different actions — Clicks, Hides, and Saves, among others — on the ads.

[https://medium.com/pinterest-engineering/pinterest-home-feed-unified-lightweight-scoring-a-two-tower-approach-b3143ac70b55](https://medium.com/pinterest-engineering/pinterest-home-feed-unified-lightweight-scoring-a-two-tower-approach-b3143ac70b55)

Features for home feed ranking:

Bias and position features: Binary and categorical features such as app type, device type, gender, bucketized positions and cluster IDs. The cluster IDs are generated by an internal algorithm mapping Pins to a pool of clusters.
User and Pin performance features: The performance (repin rate, click rate, close-up rate) of Pins and Pinners at different time granularities such as the past 3 hours, 1 day, 3 days, 30 days and 90 days.
Feedback-loop features: The empirical action rates on the platform in the last 30 minutes aggregated by overall, country, gender-cross-country. These features were included to capture fluctuations during the day.

### YouTube Video Recommendations

1. A user’s watch history is represented by a variable-length sequence of sparse video IDs which is mapped to a dense vector representation via the embeddings.
2. Search history is treated similarly to watch history - each query is tokenized into unigrams and bigrams and each token is embedded. Once averaged, the user’s tokenized, embedded queries represent a summarized dense search history.
3. we feed the age of the training example as a feature during training.
4. Modelling expected watch time

### Overall points:

1. Wide and deep model
2. three parts: candidate gen, ranking, reranking. candidate gen from graph, collab filtering, content-based 
3. ranking models: log reg, gradient boosted dt, deep factorization machine
4. Offline metrics: precision, recall,ranking metrics like ndcg etc. User engagement metrics: click through rate, watch time Online metric: A/B testing on user engagement and satisfaction metrics (as final evaluation step; iterative improvements done via offline metrics)
5. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement.
6. Engagement: Classification - click, Regression - watch time. Satisfaction: Classification - like/dislike, Regression - rating
7. Position bias: liking smth bc it appeared first on your feed.